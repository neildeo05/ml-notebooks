{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]], dtype=torch.int32)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#Lesson One - Tensors and Numpy Integrations\n",
    "'''An uninitialized matrix is declared,\n",
    "but does not contain definite known values before it is used.\n",
    "When an uninitialized matrix is created, whatever values were \n",
    "in the allocated memory at the time will appear as the initial values.'''\n",
    "\n",
    "#Uninitialized\n",
    "matrix = torch.empty(5,3)\n",
    "\n",
    "#Random\n",
    "matrix = torch.rand(5,3)\n",
    "# print(matrix)\n",
    "\n",
    "#np.zeros but contains a long\n",
    "\n",
    "x = torch.zeros(5,3, dtype=torch.long)\n",
    "# print(x)\n",
    "\n",
    "#create a tensor with manual data - set type equal to float\n",
    "x = torch.tensor([5.5,3], dtype = torch.float)\n",
    "# print(x)\n",
    "\n",
    "#create a tensor based on the previous tensor\n",
    "x = x.new_ones(3,3,dtype=torch.double)\n",
    "#fill the tensor with random doubles\n",
    "x = torch.randn_like(x, dtype=torch.double)\n",
    "#print size of array\n",
    "# x.size()\n",
    "#add two tensors of the same size\n",
    "y = torch.rand(3,3, dtype=torch.double)\n",
    "res = torch.empty(3,3)\n",
    "torch.add(x,y, out=res)\n",
    "#operation that mutates a tensor in place is post fixed with an _\n",
    "# res.copy_(y)\n",
    "\n",
    "#indexing\n",
    "# print(x)\n",
    "x[:,-1] #last column\n",
    "x = torch.rand(4,4)\n",
    "#resizing a tensor\n",
    "# (x.view(-1,8))\n",
    "\n",
    "#one element tensor\n",
    "# x = torch.randn(1)\n",
    "#prints the one item of the one element tensor\n",
    "# (x.item())\n",
    "\n",
    "#converting to a np array\n",
    "\n",
    "a = torch.zeros(3,3, dtype = torch.double)\n",
    "a.add_(2)\n",
    "b = a.numpy()\n",
    "\n",
    "#converting np array to torch tensor\n",
    "import numpy as np\n",
    "\n",
    "a = np.zeros((3,3))\n",
    "b = torch.from_numpy(a)\n",
    "print(b.to(torch.int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lesson 2 - Linear Regression\n",
    "#predicting yields of apples and oranges depending on input variables/features\n",
    "\n",
    "#in linear regression - each target variable is estimated to be a weighted sum of the input variables, offset by some constant known as bias\n",
    "#usually it is just some weights multiplied by input variables, and a constant is added to it in case there is a\n",
    "#discrepancy in the training data\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "#Inputs (temperature, rainfall, humidity)\n",
    "\n",
    "inp = np.array([[73,67,43],\n",
    "                  [91,88,64],\n",
    "                  [87,134,58],\n",
    "                   [102,43,37],\n",
    "                  [69,96,70]], dtype='double')\n",
    "trg = np.array([[56,70],\n",
    "                  [81,101],\n",
    "                  [119,133],\n",
    "                   [22,37],\n",
    "                  [103,119]], dtype='double')\n",
    "#you can get a column by doing [:,0]\n",
    "#Targets (apples, oranges) inital data before the prediction\n",
    "# targets = np.array([[56,70],[81,101],[119,133],[22,37], [103,119]], dtype='double')\n",
    "inputs = torch.from_numpy(inp)\n",
    "targets = torch.from_numpy(trg)\n",
    "\n",
    "#we can create some random weights - but since there are three input types, (temp, rainfall, hum) we need three weights\n",
    "\n",
    "\n",
    "#weights are matrices, they are random for now\n",
    "weights = torch.randn((2,3), requires_grad=True, dtype=torch.double)\n",
    "\n",
    "#biases are vectors, they are random as well. They will be the constants we add to the weights and the inps\n",
    "biases = torch.randn((2), requires_grad=True, dtype=torch.double)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -7839.6372, -11021.6460,  -6184.9131],\n",
      "        [-13278.6356, -14979.5404,  -9099.3329]], dtype=torch.float64)\n",
      "tensor([ -98.9779, -159.3000], dtype=torch.float64)\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]], dtype=torch.float64) tensor([0., 0.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(x * y)\n",
    "\n",
    "# # @ gives the final value of a matrix multiplication set\n",
    "# print(x @ y)\n",
    "# print(weights.t())\n",
    "# print(inp)\n",
    "\n",
    "\n",
    "#structure of weights matrix = (inputs values, target values)\n",
    "\n",
    "def model(x):\n",
    "    return x @ weights.t() + biases \n",
    "predictions = model(inputs)\n",
    "\n",
    "def MSE(t1, t2):\n",
    "    #evaluate how well model is performing\n",
    "    diff = t1 - t2\n",
    "    diff_sqr = diff ** 2\n",
    "    #get the average of the squared matrix - sum of all values / len of matrix\n",
    "    #numel gets number of elements , or diff.size()[0] * diff.size()[1]\n",
    "    return torch.sum(diff_sqr) / torch.numel(diff_sqr)\n",
    "loss = MSE(predictions, targets)\n",
    "loss\n",
    "#interpretation: On average, each of model's predictions is off by the sqrt of the loss(for example 255)\n",
    "#tells how much info it is losing - when random - very much loss\n",
    "loss.backward()\n",
    "#everytime you call backwards it keeps adding \n",
    "print(weights.grad)\n",
    "print(biases.grad)\n",
    "\n",
    "#Loss is a quadratic function of the weights and biases\n",
    "#if you increase / decrease the weights too much then the loss will be too high \n",
    "\n",
    "\n",
    "#Positive Weights and Positive Gradient\n",
    "#make sure to decrease slightly, otherwise the loss will be bigger\n",
    "#increasing the elements value will increase the loss\n",
    "#However, if you decrease the element's value, then the loss will decrease\n",
    "\n",
    "#Positive Weights and Negative Gradient\n",
    "#make sure to decrease slightly, otherwise the loss will be bigger\n",
    "#increasing the element will decrease the loss\n",
    "#decreasing the elemnt will increase the loss\n",
    "\n",
    "#Since right now, our weights and biases are all random, we have to increase the weights value in order to decrease the loss\n",
    "weights.grad.zero_()\n",
    "biases.grad.zero_()\n",
    "print(weights.grad, biases.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
