{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression is a supervised regression model, but logistic regression is a supervised classification model\n",
    "#predicting the value of a certain image - classifying it based on training data\n",
    "\n",
    "import torch\n",
    "#The torchvision package consists of popular datasets, model architectures, and common image transformations \n",
    "#for computer vision.\n",
    "import torchvision\n",
    "#import the MNIST dataset, which has all of the images - very popular, like the iris dataset\n",
    "from torchvision.datasets import MNIST\n",
    "import numpy\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training set and testing set\n",
    "dataset = MNIST(root=\"data/\", train=True, transform = transforms.ToTensor())\n",
    "test_dataset = MNIST(root=\"data/\", train=False, transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: data/\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: data/\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    }
   ],
   "source": [
    "#validation set\n",
    "perms = (np.random.permutation(len(dataset)))\n",
    "san_per = 0.2\n",
    "indices = int(len(dataset)*san_per)\n",
    "print(indices)\n",
    "train_indices = perms[indices:]\n",
    "val_indices = perms[:indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training dataloader\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "train_dl = DataLoader(dataset, 100, sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation dataloader\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "val_dl = DataLoader(dataset, 100, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "\n",
    "#the forward function just computes the outputs based on the inputs, or xb in this case\n",
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #the reason for these inputs and outputs are because we want to flatten out the image, and then also \n",
    "        #the outputs are for the probabilites of the item being from 0 - 9\n",
    "        self.linear = nn.Linear(28*28, 10)\n",
    "        \n",
    "    def forward(self,xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MnistModel()\n",
    "\n",
    "#does softmax on its own, so the items can be changed to probabilities\n",
    "loss_fn = F.cross_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_on_batch(loss_fn, model, xb, yb, opt):\n",
    "    outputs = model(xb)\n",
    "    loss = loss_fn(outputs, yb)\n",
    "    \n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model training\n",
    "def train(epochs, loss_fn, model,train_dl, lr):\n",
    "    opt = torch.optim.SGD(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        for xb, yb in train_dl:\n",
    "            loss_on_batch(loss_fn, model, xb, yb, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(5, F.cross_entropy, model, train_dl, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    probs, preds = torch.max(outputs, dim=1)\n",
    "    return torch.sum(preds == labels).item()/len(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "tensor(10.9748)\n",
      "0.79\n",
      "tensor(35.9415)\n",
      "0.78\n",
      "tensor(43.8578)\n",
      "0.81\n",
      "tensor(44.6358)\n",
      "0.82\n",
      "tensor(30.5636)\n",
      "0.83\n",
      "tensor(36.2815)\n",
      "0.85\n",
      "tensor(23.5338)\n",
      "0.83\n",
      "tensor(32.3816)\n",
      "0.87\n",
      "tensor(18.0078)\n",
      "0.79\n",
      "tensor(44.8689)\n",
      "0.8\n",
      "tensor(38.5423)\n",
      "0.75\n",
      "tensor(67.8172)\n",
      "0.79\n",
      "tensor(56.3568)\n",
      "0.82\n",
      "tensor(21.7528)\n",
      "0.88\n",
      "tensor(22.2279)\n",
      "0.82\n",
      "tensor(32.2955)\n",
      "0.77\n",
      "tensor(61.8874)\n",
      "0.84\n",
      "tensor(28.3571)\n",
      "0.75\n",
      "tensor(36.5287)\n",
      "0.79\n",
      "tensor(32.7579)\n",
      "0.79\n",
      "tensor(36.5748)\n",
      "0.81\n",
      "tensor(38.4646)\n",
      "0.83\n",
      "tensor(27.7541)\n",
      "0.79\n",
      "tensor(43.1201)\n",
      "0.78\n",
      "tensor(46.5797)\n",
      "0.82\n",
      "tensor(27.0292)\n",
      "0.82\n",
      "tensor(38.4494)\n",
      "0.83\n",
      "tensor(35.6789)\n",
      "0.76\n",
      "tensor(54.1768)\n",
      "0.84\n",
      "tensor(33.2769)\n",
      "0.83\n",
      "tensor(39.9563)\n",
      "0.79\n",
      "tensor(39.6237)\n",
      "0.83\n",
      "tensor(39.1105)\n",
      "0.8\n",
      "tensor(36.5150)\n",
      "0.81\n",
      "tensor(33.0137)\n",
      "0.83\n",
      "tensor(25.5974)\n",
      "0.66\n",
      "tensor(65.5646)\n",
      "0.78\n",
      "tensor(39.7986)\n",
      "0.84\n",
      "tensor(26.9128)\n",
      "0.75\n",
      "tensor(38.5633)\n",
      "0.8\n",
      "tensor(38.2426)\n",
      "0.85\n",
      "tensor(24.1107)\n",
      "0.76\n",
      "tensor(49.3490)\n",
      "0.81\n",
      "tensor(33.9399)\n",
      "0.83\n",
      "tensor(27.3094)\n",
      "0.81\n",
      "tensor(39.6623)\n",
      "0.81\n",
      "tensor(47.3043)\n",
      "0.77\n",
      "tensor(49.0985)\n",
      "0.85\n",
      "tensor(28.7631)\n",
      "0.88\n",
      "tensor(25.7334)\n",
      "0.8\n",
      "tensor(32.2600)\n",
      "0.85\n",
      "tensor(25.1697)\n",
      "0.75\n",
      "tensor(57.4065)\n",
      "0.85\n",
      "tensor(34.5226)\n",
      "0.79\n",
      "tensor(41.6142)\n",
      "0.84\n",
      "tensor(22.5675)\n",
      "0.79\n",
      "tensor(43.9308)\n",
      "0.88\n",
      "tensor(13.9479)\n",
      "0.79\n",
      "tensor(46.2054)\n",
      "0.84\n",
      "tensor(34.8452)\n",
      "0.79\n",
      "tensor(42.0019)\n",
      "0.8\n",
      "tensor(36.1453)\n",
      "0.91\n",
      "tensor(27.0840)\n",
      "0.86\n",
      "tensor(28.8695)\n",
      "0.88\n",
      "tensor(17.6650)\n",
      "0.87\n",
      "tensor(17.1461)\n",
      "0.84\n",
      "tensor(24.7595)\n",
      "0.81\n",
      "tensor(38.1434)\n",
      "0.87\n",
      "tensor(26.3802)\n",
      "0.81\n",
      "tensor(46.7891)\n",
      "0.83\n",
      "tensor(37.6964)\n",
      "0.79\n",
      "tensor(61.9018)\n",
      "0.76\n",
      "tensor(53.1552)\n",
      "0.75\n",
      "tensor(44.6620)\n",
      "0.76\n",
      "tensor(52.7973)\n",
      "0.83\n",
      "tensor(34.5971)\n",
      "0.83\n",
      "tensor(29.5690)\n",
      "0.88\n",
      "tensor(26.4259)\n",
      "0.84\n",
      "tensor(32.4472)\n",
      "0.76\n",
      "tensor(38.4718)\n",
      "0.76\n",
      "tensor(55.1605)\n",
      "0.76\n",
      "tensor(57.4582)\n",
      "0.88\n",
      "tensor(13.9543)\n",
      "0.85\n",
      "tensor(30.7735)\n",
      "0.8\n",
      "tensor(50.0881)\n",
      "0.86\n",
      "tensor(28.8474)\n",
      "0.79\n",
      "tensor(49.7778)\n",
      "0.82\n",
      "tensor(27.4044)\n",
      "0.67\n",
      "tensor(56.2128)\n",
      "0.8\n",
      "tensor(38.6214)\n",
      "0.82\n",
      "tensor(39.1158)\n",
      "0.82\n",
      "tensor(36.0564)\n",
      "0.83\n",
      "tensor(43.6106)\n",
      "0.86\n",
      "tensor(26.4498)\n",
      "0.84\n",
      "tensor(31.6996)\n",
      "0.82\n",
      "tensor(26.3181)\n",
      "0.83\n",
      "tensor(41.2047)\n",
      "0.92\n",
      "tensor(16.2489)\n",
      "0.83\n",
      "tensor(39.1863)\n",
      "0.72\n",
      "tensor(46.3394)\n",
      "0.78\n",
      "tensor(69.3447)\n",
      "0.84\n",
      "tensor(33.3507)\n",
      "0.78\n",
      "tensor(45.8626)\n",
      "0.78\n",
      "tensor(58.3893)\n",
      "0.78\n",
      "tensor(44.3344)\n",
      "0.76\n",
      "tensor(40.4879)\n",
      "0.72\n",
      "tensor(56.8649)\n",
      "0.79\n",
      "tensor(44.6045)\n",
      "0.86\n",
      "tensor(24.8732)\n",
      "0.82\n",
      "tensor(34.1733)\n",
      "0.78\n",
      "tensor(53.8860)\n",
      "0.73\n",
      "tensor(63.9727)\n",
      "0.85\n",
      "tensor(30.1777)\n",
      "0.83\n",
      "tensor(39.0485)\n",
      "0.78\n",
      "tensor(61.7389)\n",
      "0.78\n",
      "tensor(43.4631)\n",
      "0.8\n",
      "tensor(48.0492)\n",
      "0.81\n",
      "tensor(46.5013)\n",
      "0.78\n",
      "tensor(32.3987)\n",
      "0.76\n",
      "tensor(37.9947)\n",
      "0.8098333333333333\n"
     ]
    }
   ],
   "source": [
    "avg_acc = []\n",
    "def validate(valid_dl, model):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, yb in valid_dl:\n",
    "            preds = model(xb)\n",
    "            print(accuracy(preds, yb))\n",
    "            print(loss_fn(preds, yb))\n",
    "            avg_acc.append(accuracy(preds,yb))\n",
    "\n",
    "    return sum(avg_acc)/len(avg_acc)\n",
    "\n",
    "#after running the model through the validation dataset, this was the average accuracy\n",
    "print(validate(val_dl, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MNIST(root = 'data/', train = False, transform = transforms.ToTensor())\n",
    "def predict_image(img, model):\n",
    "    xb = img.unsqueeze(0)\n",
    "    yb = model(xb)\n",
    "    prob, preds = torch.max(yb, dim = 1 )\n",
    "    \n",
    "    print(preds[0].item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12ee610b8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = 7891\n",
    "predict_image(test_dataset[x][0], model)\n",
    "print(test_dataset[x][1])\n",
    "\n",
    "ten, lab = test_dataset[x]\n",
    "\n",
    "plt.imshow(ten[0],cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
