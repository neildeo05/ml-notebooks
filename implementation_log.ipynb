{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression is a supervised regression model, but logistic regression is a supervised classification model\n",
    "#predicting the value of a certain image - classifying it based on training data\n",
    "\n",
    "import torch\n",
    "#The torchvision package consists of popular datasets, model architectures, and common image transformations \n",
    "#for computer vision.\n",
    "import torchvision\n",
    "#import the MNIST dataset, which has all of the images - very popular, like the iris dataset\n",
    "from torchvision.datasets import MNIST\n",
    "import numpy\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training set and testing set\n",
    "dataset = MNIST(root=\"data/\", train=True, transform = transforms.ToTensor())\n",
    "test_dataset = MNIST(root=\"data/\", train=False, transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: data/\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: data/\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    }
   ],
   "source": [
    "#validation set\n",
    "perms = (np.random.permutation(len(dataset)))\n",
    "san_per = 0.2\n",
    "indices = int(len(dataset)*san_per)\n",
    "print(indices)\n",
    "train_indices = perms[indices:]\n",
    "val_indices = perms[:indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training dataloader\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "train_dl = DataLoader(dataset, 100, sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation dataloader\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "val_dl = DataLoader(dataset, 100, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "\n",
    "#the forward function just computes the outputs based on the inputs, or xb in this case\n",
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #the reason for these inputs and outputs are because we want to flatten out the image, and then also \n",
    "        #the outputs are for the probabilites of the item being from 0 - 9\n",
    "        self.linear = nn.Linear(28*28, 10)\n",
    "        \n",
    "    def forward(self,xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MnistModel()\n",
    "\n",
    "#does softmax on its own, so the items can be changed to probabilities\n",
    "loss_fn = F.cross_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_on_batch(loss_fn, model, xb, yb, opt):\n",
    "    outputs = model(xb)\n",
    "    loss = loss_fn(outputs, yb)\n",
    "    \n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model training\n",
    "def train(epochs, loss_fn, model,train_dl, lr):\n",
    "    opt = torch.optim.SGD(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        for xb, yb in train_dl:\n",
    "            loss_on_batch(loss_fn, model, xb, yb, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(5, F.cross_entropy, model, train_dl, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    probs, preds = torch.max(outputs, dim=1)\n",
    "    return torch.sum(preds == labels).item()/len(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "tensor(10.9748)\n",
      "0.79\n",
      "tensor(35.9415)\n",
      "0.78\n",
      "tensor(43.8578)\n",
      "0.81\n",
      "tensor(44.6358)\n",
      "0.82\n",
      "tensor(30.5636)\n",
      "0.83\n",
      "tensor(36.2815)\n",
      "0.85\n",
      "tensor(23.5338)\n",
      "0.83\n",
      "tensor(32.3816)\n",
      "0.87\n",
      "tensor(18.0078)\n",
      "0.79\n",
      "tensor(44.8689)\n",
      "0.8\n",
      "tensor(38.5423)\n",
      "0.75\n",
      "tensor(67.8172)\n",
      "0.79\n",
      "tensor(56.3568)\n",
      "0.82\n",
      "tensor(21.7528)\n",
      "0.88\n",
      "tensor(22.2279)\n",
      "0.82\n",
      "tensor(32.2955)\n",
      "0.77\n",
      "tensor(61.8874)\n",
      "0.84\n",
      "tensor(28.3571)\n",
      "0.75\n",
      "tensor(36.5287)\n",
      "0.79\n",
      "tensor(32.7579)\n",
      "0.79\n",
      "tensor(36.5748)\n",
      "0.81\n",
      "tensor(38.4646)\n",
      "0.83\n",
      "tensor(27.7541)\n",
      "0.79\n",
      "tensor(43.1201)\n",
      "0.78\n",
      "tensor(46.5797)\n",
      "0.82\n",
      "tensor(27.0292)\n",
      "0.82\n",
      "tensor(38.4494)\n",
      "0.83\n",
      "tensor(35.6789)\n",
      "0.76\n",
      "tensor(54.1768)\n",
      "0.84\n",
      "tensor(33.2769)\n",
      "0.83\n",
      "tensor(39.9563)\n",
      "0.79\n",
      "tensor(39.6237)\n",
      "0.83\n",
      "tensor(39.1105)\n",
      "0.8\n",
      "tensor(36.5150)\n",
      "0.81\n",
      "tensor(33.0137)\n",
      "0.83\n",
      "tensor(25.5974)\n",
      "0.66\n",
      "tensor(65.5646)\n",
      "0.78\n",
      "tensor(39.7986)\n",
      "0.84\n",
      "tensor(26.9128)\n",
      "0.75\n",
      "tensor(38.5633)\n",
      "0.8\n",
      "tensor(38.2426)\n",
      "0.85\n",
      "tensor(24.1107)\n",
      "0.76\n",
      "tensor(49.3490)\n",
      "0.81\n",
      "tensor(33.9399)\n",
      "0.83\n",
      "tensor(27.3094)\n",
      "0.81\n",
      "tensor(39.6623)\n",
      "0.81\n",
      "tensor(47.3043)\n",
      "0.77\n",
      "tensor(49.0985)\n",
      "0.85\n",
      "tensor(28.7631)\n",
      "0.88\n",
      "tensor(25.7334)\n",
      "0.8\n",
      "tensor(32.2600)\n",
      "0.85\n",
      "tensor(25.1697)\n",
      "0.75\n",
      "tensor(57.4065)\n",
      "0.85\n",
      "tensor(34.5226)\n",
      "0.79\n",
      "tensor(41.6142)\n",
      "0.84\n",
      "tensor(22.5675)\n",
      "0.79\n",
      "tensor(43.9308)\n",
      "0.88\n",
      "tensor(13.9479)\n",
      "0.79\n",
      "tensor(46.2054)\n",
      "0.84\n",
      "tensor(34.8452)\n",
      "0.79\n",
      "tensor(42.0019)\n",
      "0.8\n",
      "tensor(36.1453)\n",
      "0.91\n",
      "tensor(27.0840)\n",
      "0.86\n",
      "tensor(28.8695)\n",
      "0.88\n",
      "tensor(17.6650)\n",
      "0.87\n",
      "tensor(17.1461)\n",
      "0.84\n",
      "tensor(24.7595)\n",
      "0.81\n",
      "tensor(38.1434)\n",
      "0.87\n",
      "tensor(26.3802)\n",
      "0.81\n",
      "tensor(46.7891)\n",
      "0.83\n",
      "tensor(37.6964)\n",
      "0.79\n",
      "tensor(61.9018)\n",
      "0.76\n",
      "tensor(53.1552)\n",
      "0.75\n",
      "tensor(44.6620)\n",
      "0.76\n",
      "tensor(52.7973)\n",
      "0.83\n",
      "tensor(34.5971)\n",
      "0.83\n",
      "tensor(29.5690)\n",
      "0.88\n",
      "tensor(26.4259)\n",
      "0.84\n",
      "tensor(32.4472)\n",
      "0.76\n",
      "tensor(38.4718)\n",
      "0.76\n",
      "tensor(55.1605)\n",
      "0.76\n",
      "tensor(57.4582)\n",
      "0.88\n",
      "tensor(13.9543)\n",
      "0.85\n",
      "tensor(30.7735)\n",
      "0.8\n",
      "tensor(50.0881)\n",
      "0.86\n",
      "tensor(28.8474)\n",
      "0.79\n",
      "tensor(49.7778)\n",
      "0.82\n",
      "tensor(27.4044)\n",
      "0.67\n",
      "tensor(56.2128)\n",
      "0.8\n",
      "tensor(38.6214)\n",
      "0.82\n",
      "tensor(39.1158)\n",
      "0.82\n",
      "tensor(36.0564)\n",
      "0.83\n",
      "tensor(43.6106)\n",
      "0.86\n",
      "tensor(26.4498)\n",
      "0.84\n",
      "tensor(31.6996)\n",
      "0.82\n",
      "tensor(26.3181)\n",
      "0.83\n",
      "tensor(41.2047)\n",
      "0.92\n",
      "tensor(16.2489)\n",
      "0.83\n",
      "tensor(39.1863)\n",
      "0.72\n",
      "tensor(46.3394)\n",
      "0.78\n",
      "tensor(69.3447)\n",
      "0.84\n",
      "tensor(33.3507)\n",
      "0.78\n",
      "tensor(45.8626)\n",
      "0.78\n",
      "tensor(58.3893)\n",
      "0.78\n",
      "tensor(44.3344)\n",
      "0.76\n",
      "tensor(40.4879)\n",
      "0.72\n",
      "tensor(56.8649)\n",
      "0.79\n",
      "tensor(44.6045)\n",
      "0.86\n",
      "tensor(24.8732)\n",
      "0.82\n",
      "tensor(34.1733)\n",
      "0.78\n",
      "tensor(53.8860)\n",
      "0.73\n",
      "tensor(63.9727)\n",
      "0.85\n",
      "tensor(30.1777)\n",
      "0.83\n",
      "tensor(39.0485)\n",
      "0.78\n",
      "tensor(61.7389)\n",
      "0.78\n",
      "tensor(43.4631)\n",
      "0.8\n",
      "tensor(48.0492)\n",
      "0.81\n",
      "tensor(46.5013)\n",
      "0.78\n",
      "tensor(32.3987)\n",
      "0.76\n",
      "tensor(37.9947)\n",
      "0.8098333333333333\n"
     ]
    }
   ],
   "source": [
    "avg_acc = []\n",
    "def validate(valid_dl, model):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, yb in valid_dl:\n",
    "            preds = model(xb)\n",
    "            print(accuracy(preds, yb))\n",
    "            print(loss_fn(preds, yb))\n",
    "            avg_acc.append(accuracy(preds,yb))\n",
    "\n",
    "    return sum(avg_acc)/len(avg_acc)\n",
    "\n",
    "#after running the model through the validation dataset, this was the average accuracy\n",
    "print(validate(val_dl, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MNIST(root = 'data/', train = False, transform = transforms.ToTensor())\n",
    "def predict_image(img, model):\n",
    "    xb = img.unsqueeze(0)\n",
    "    yb = model(xb)\n",
    "    prob, preds = torch.max(yb, dim = 1 )\n",
    "    \n",
    "    print(preds[0].item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-536de3cdad28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7891\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict_image' is not defined"
     ]
    }
   ],
   "source": [
    "x = 7891\n",
    "predict_image(test_dataset[x][0], model)\n",
    "print(test_dataset[x][1])\n",
    "\n",
    "ten, lab = test_dataset[x]\n",
    "\n",
    "plt.imshow(ten[0],cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
