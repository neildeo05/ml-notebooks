{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression is a supervised regression model, but logistic regression is a supervised classification model\n",
    "#predicting the value of a certain image - classifying it based on training data\n",
    "\n",
    "import torch\n",
    "#The torchvision package consists of popular datasets, model architectures, and common image transformations \n",
    "#for computer vision.\n",
    "import torchvision\n",
    "#import the MNIST dataset, which has all of the images - very popular, like the iris dataset\n",
    "from torchvision.datasets import MNIST\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actually download the dataset inside the data directory\n",
    "dataset = MNIST(root='data/', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look in your ml-notebooks directory - there is a directory called data. This is where we will store all of \n",
    "#the datasets that need separate files. The linear regression model was trained on very small amounts of data\n",
    "#and they were both tensors, so its easier to just store the data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "#The dataset has 60,000 images! We will have an ample amount of training data\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "#this dows not need to be download\n",
    "test = MNIST(root='data/', train=False)\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<PIL.Image.Image image mode=L size=28x28 at 0x126EE9240>, 5)\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "image, label = dataset[1]\n",
    "\n",
    "plt.imshow(image, cmap='gray')\n",
    "print(\"Label:\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "dataset = MNIST(root=\"data/\", train=True, transform=transforms.ToTensor())\n",
    "ten, lab = dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0039, 0.6039, 0.9922, 0.3529, 0.0000],\n",
      "         [0.0000, 0.5451, 0.9922, 0.7451, 0.0078],\n",
      "         [0.0000, 0.0431, 0.7451, 0.9922, 0.2745],\n",
      "         [0.0000, 0.0000, 0.1373, 0.9451, 0.8824],\n",
      "         [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]]])\n"
     ]
    }
   ],
   "source": [
    "#Print the pixels at the 10th to 15th y axis \n",
    "print(ten[:,10:15, 10:15])\n",
    "#1 represents white, and 0 represents black - the some of these values are just in between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12b6cd080>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAJRElEQVR4nO3dz2ucBR7H8c9n04qiCx7qQZrSiohsEVahFKEHoQjWKnpVqF7UXFaoIIge/QfEi5egYsFSEfQg6iIFFRGsGjUWu1GoPxaLQncprXpRaj97mGHpuknzzHSeeeb58n5BIJMZMh9K3n1mJuEZJxGAOv7U9QAAk0XUQDFEDRRD1EAxRA0Us6GNb2q7Ny+pb926tesJI9m0aVPXE0by7bffdj2hsVOnTnU9YSRJvNrX3cavtGzHXvX+Zs7i4mLXE0by4IMPdj1hJPv27et6QmMHDx7sesJI1oqah99AMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxjaK2vcf2V7aP23687VEAxrdu1LbnJD0j6XZJ2yXda3t728MAjKfJkXqnpONJvknym6SXJN3d7iwA42oS9WZJ3593+cTwa//D9oLtJdtLkxoHYHRNThG82hkL/+8UpEkWJS1K/TpFMFBNkyP1CUlbzrs8L+mHduYAuFhNov5Y0nW2r7F9iaR7JL3W7iwA41r34XeSs7YflvSWpDlJzyc51voyAGNp9LY7Sd6U9GbLWwBMAH9RBhRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMY1OkjCOpB/nHjxz5kzXE0p76KGHup7Q2KFDh7qe0Ni5c+fWvI4jNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UMy6Udt+3vZJ219MYxCAi9PkSP2CpD0t7wAwIetGneQ9SaemsAXABPCcGihmYmcTtb0gaWFS3w/AeCYWdZJFSYuSZLsf5wcGCuLhN1BMk19pHZL0gaTrbZ+w/UD7swCMa92H30nuncYQAJPBw2+gGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBopxMvnTifXpHGWXX3551xNG8sYbb3Q9YSS33HJL1xMau+2227qe0NiRI0d05swZr3YdR2qgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKWTdq21tsv2N7xfYx2/unMQzAeDY0uM1ZSY8m+dT2nyV9Yvtwkn+0vA3AGNY9Uif5Mcmnw89/lrQiaXPbwwCMp8mR+r9sb5N0k6QPV7luQdLCRFYBGFvjqG1fIekVSY8k+emP1ydZlLQ4vG1vThEMVNPo1W/bGzUI+mCSV9udBOBiNHn125Kek7SS5Kn2JwG4GE2O1Lsk3Sdpt+3l4cfelncBGNO6z6mTvC9p1bf3ADB7+IsyoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKcTL5cwRy4sH2XHvttV1PGMny8nLXExo7ffp01xMa27t3r44ePbrqyUs4UgPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8WsG7XtS21/ZPtz28dsPzmNYQDGs6HBbX6VtDvJL7Y3Snrf9t+THGl5G4AxrBt1Bicx+2V4cePwg3OQATOq0XNq23O2lyWdlHQ4yYftzgIwrkZRJ/k9yY2S5iXttH3DH29je8H2ku2lSY8E0NxIr34nOS3pXUl7VrluMcmOJDsmtA3AGJq8+n2V7SuHn18m6VZJX7Y9DMB4mrz6fbWkA7bnNPhP4OUkr7c7C8C4mrz6fVTSTVPYAmAC+IsyoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKaXLmE8yQr7/+uusJI7n//vu7ntDYgQMHup7Q2IYNa6fLkRoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiGkdte872Z7Zfb3MQgIszypF6v6SVtoYAmIxGUduel3SHpGfbnQPgYjU9Uj8t6TFJ59a6ge0F20u2lyayDMBY1o3a9p2STib55EK3S7KYZEeSHRNbB2BkTY7UuyTdZfs7SS9J2m37xVZXARjbulEneSLJfJJtku6R9HaSfa0vAzAWfk8NFDPS2+4keVfSu60sATARHKmBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGijGSSb/Te1/SfrnhL/tJkn/nvD3bFOf9vZpq9SvvW1t3ZrkqtWuaCXqNthe6tOZSvu0t09bpX7t7WIrD7+BYogaKKZPUS92PWBEfdrbp61Sv/ZOfWtvnlMDaKZPR2oADRA1UEwvora9x/ZXto/bfrzrPRdi+3nbJ21/0fWW9djeYvsd2yu2j9ne3/Wmtdi+1PZHtj8fbn2y601N2J6z/Znt16d1nzMfte05Sc9Iul3Sdkn32t7e7aoLekHSnq5HNHRW0qNJ/iLpZkl/m+F/218l7U7yV0k3Stpj++aONzWxX9LKNO9w5qOWtFPS8STfJPlNg3fevLvjTWtK8p6kU13vaCLJj0k+HX7+swY/fJu7XbW6DPwyvLhx+DHTr/Lanpd0h6Rnp3m/fYh6s6Tvz7t8QjP6g9dntrdJuknSh90uWdvwoeyypJOSDieZ2a1DT0t6TNK5ad5pH6L2Kl+b6f+h+8b2FZJekfRIkp+63rOWJL8nuVHSvKSdtm/oetNabN8p6WSST6Z9332I+oSkLeddnpf0Q0dbyrG9UYOgDyZ5tes9TSQ5rcG7r87yaxe7JN1l+zsNnjLutv3iNO64D1F/LOk629fYvkSDN75/reNNJdi2pOckrSR5qus9F2L7KttXDj+/TNKtkr7sdtXakjyRZD7JNg1+Zt9Osm8a9z3zUSc5K+lhSW9p8ELOy0mOdbtqbbYPSfpA0vW2T9h+oOtNF7BL0n0aHEWWhx97ux61hqslvWP7qAb/0R9OMrVfE/UJfyYKFDPzR2oAoyFqoBiiBoohaqAYogaKIWqgGKIGivkPGaruA1eRIiMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(ten[0,10:15,10:15], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split set into 3 models - training set, validation set, test set\n",
    "#training set - used to train the model , compute the loss, adjust the weights \n",
    "#validation set - evaluate the model while training, adjust parameters like the learning rate, and pick the best model\n",
    "#test set - used to compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_indices(n, val_pct):\n",
    "    #split the val into a training dataset and a validation dataset\n",
    "    n_val = int(val_pct*n)\n",
    "    \n",
    "    #random images for the validation set and for the training dataset \n",
    "    idxs = np.random.permutation(n)\n",
    "    \n",
    "    #returns the first n_vals in a tuple, so basically a fraction of the entire dataset, and splits the rest for\n",
    "    #the training set\n",
    "    return idxs[n_val:], idxs[:n_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, val_indices = split_indices(len(dataset), 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomize the data in order to remove bias\n",
    "\n",
    "#sample randomly from indices\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = 100\n",
    "\n",
    "#randomize the data samples\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "train_loader= DataLoader(dataset, batches, sampler=train_sampler)\n",
    "\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "val_loader = DataLoader(dataset, batches, sampler=val_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model\n",
    "\n",
    "#The logistic regression model is very similar to the linear regression model\n",
    "\n",
    "#there are weights and there are biases - output is obtained by using matrix operations\n",
    "\n",
    "#we can use nn.Linear to crreate the model - and flattened to a vector of size 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "#the output of each image is a vector of size 10, with each element of the vector signifying the probability of \n",
    "#a particular target label - 0 to 9\n",
    "\n",
    "input_size = 28*28 #flatten the image\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "# model = nn.Linear(input_size, num_classes)\n",
    "\n",
    "#super similar to the linear regression model - but this time the inputs and the outputs are way more\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for imgs, labels in train_loader:\n",
    "#     print(labels)\n",
    "#     print(imgs.shape)\n",
    "#     outputs = model(imgs)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 10]) tensor([[ 0.1275,  0.3639,  0.0733, -0.0032,  0.0215, -0.2822, -0.1885, -0.0975,\n",
      "         -0.1118, -0.0448]])\n"
     ]
    }
   ],
   "source": [
    "#Since the linear method is expecting a vector, we can't just pass in a tensor that has 3 dimensions, \n",
    "#we need to flatten it out - we can do this by creating a model class that inherists from the nn.module class\n",
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        #putting -1 allows us to use any batch size\n",
    "        xb=xb.reshape(-1,784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "model = MnistModel()\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    outputs = model(images)\n",
    "    break\n",
    "    \n",
    "print(outputs.shape, outputs[:1].data) \n",
    "\n",
    "\n",
    "#the outputs are the random result of passing in the images into the model. However, we need them to represent\n",
    "#a probability, which is between 1 and 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax function\n",
    "#probability is boosted up, tries to push it towards one output\n",
    "#divide by sum to end up with probabilities\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "#these sum up to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 7, 1, 3, 7, 2, 7, 1, 3, 1, 7, 1, 3, 3, 3, 7, 3, 3, 7, 3, 3, 1, 3,\n",
      "        3, 1, 3, 3, 3, 1, 7, 2, 1, 2, 3, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 2, 1,\n",
      "        1, 1, 1, 1, 2, 3, 3, 1, 9, 3, 2, 1, 0, 1, 0, 3, 1, 1, 3, 3, 1, 8, 7, 1,\n",
      "        2, 5, 1, 2, 1, 3, 5, 1, 1, 2, 2, 1, 1, 1, 1, 3, 3, 3, 3, 1, 1, 1, 1, 1,\n",
      "        0, 5, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "max_probs, preds = torch.max(probs, dim=1)\n",
    "#index of the max probability\n",
    "print(preds)\n",
    "# print(max_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 4, 3, 1, 4, 7, 6, 9, 0, 6, 1, 3, 8, 6, 6, 8, 9, 9, 6, 9, 7, 6, 5, 2,\n",
       "        9, 3, 5, 2, 2, 1, 9, 4, 7, 9, 2, 9, 4, 7, 0, 7, 3, 9, 3, 9, 5, 6, 2, 7,\n",
       "        4, 5, 5, 4, 2, 3, 8, 1, 1, 8, 4, 2, 3, 1, 4, 9, 4, 8, 6, 8, 4, 1, 5, 9,\n",
       "        2, 3, 0, 1, 5, 6, 0, 0, 2, 6, 2, 3, 5, 5, 2, 8, 8, 6, 7, 0, 8, 7, 4, 1,\n",
       "        0, 6, 9, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what the number is in the index\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(l1, l2):\n",
    "    #l1 is the predictions, and l2 is the labels\n",
    "    \n",
    "    #the == operator checks which if the numbers line up\n",
    "    \n",
    "    #in total, 8 of the 100 numbers were matching. This is pretty bad - especially when you calculate the ratio\n",
    "    return torch.sum(l1 == l2).item()/len(l1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the model has 8% accuracy\n",
    "accuracy(preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cannot be used for gradient descent - mse is differentiable but torch.max() is not differentiable\n",
    "#the accuracy function is not looking at the actual probabilty \n",
    "\n",
    "\n",
    "#a good metric for the loss function is corss entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3444, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross Entropy\n",
    "\n",
    "#find where there is a correct value and then take the negative natural log of that value, \n",
    "\n",
    "loss_fn = F.cross_entropy\n",
    "#instead of passing in preds, which is softmaxed, we pass in the bare value, and it performs softmax on it\n",
    "loss = loss_fn(outputs,labels)\n",
    "\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def calc_accuracy(val):\n",
    "    return math.exp(-val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0959083015922264"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#holy shit thats doo doo\n",
    "calc_accuracy(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.0226, -0.0066, -0.0253,  ...,  0.0092, -0.0098, -0.0002],\n",
      "        [-0.0244, -0.0302, -0.0278,  ..., -0.0201,  0.0049, -0.0121],\n",
      "        [ 0.0241,  0.0071,  0.0189,  ..., -0.0245,  0.0092, -0.0114],\n",
      "        ...,\n",
      "        [ 0.0047, -0.0147,  0.0222,  ...,  0.0316,  0.0196,  0.0198],\n",
      "        [ 0.0109,  0.0239, -0.0029,  ..., -0.0238, -0.0100, -0.0052],\n",
      "        [-0.0147, -0.0051, -0.0225,  ..., -0.0128, -0.0027,  0.0199]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0119,  0.0084, -0.0297, -0.0348, -0.0138, -0.0282,  0.0102, -0.0052,\n",
      "        -0.0109,  0.0053], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "#weights and biases\n",
    "print(list(model.parameters()))\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.001\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the loss on a batch\n",
    "def loss_batch(model, loss_fn, xb, yb, opt=None, metric=None):\n",
    "    #define the intial predictions\n",
    "    preds = model(xb)\n",
    "    #calculate the loss\n",
    "    loss = loss_fn(preds, yb)\n",
    "#     print(loss)\n",
    "    \n",
    "    if opt is not None:\n",
    "        #evaluate the gradients\n",
    "        loss.backward()\n",
    "        #re-pass the parameters in the optimizer\n",
    "        opt.step()\n",
    "        #set the gradients back to zero\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    \n",
    "    metric_result = None\n",
    "    #a metric is either the accuracy function or the cross entropy function\n",
    "    if metric is not None:\n",
    "        #calculate the accuracy of the predictions and yb, which is the y value of the batch\n",
    "        \n",
    "        \n",
    "        #this is where we are calling the metric\n",
    "        metric_result = metric(preds, yb)\n",
    "        \n",
    "    return loss.item(), len(xb), metric_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, valid_dl, metric=None):\n",
    "    #no computation of gradients - because this is used for the validation dataset\n",
    "    with torch.no_grad():\n",
    "        results = [loss_batch(model, loss_fn, xb, yb, metric = metric) for xb,yb in valid_dl]\n",
    "        #calculate the loss on the batch, which returns the cross entropy loss, and the accuracy prediction\n",
    "#         print(results)\n",
    "        \n",
    "        losses, nums, metrics = zip(*results)\n",
    "        total = np.sum(nums)\n",
    "        avg_loss = np.sum(np.multiply(losses, nums)) / total\n",
    "        avg_metric = None\n",
    "        if metric is not None:\n",
    "            avg_metric = np.sum(np.multiply(metrics, nums)) / total\n",
    "            \n",
    "    return avg_loss, total, avg_metric\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.sum(preds == labels).item()/len(labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8997616132100423 0.6371666666666667\n"
     ]
    }
   ],
   "source": [
    "val_loss, total, val_acc = evaluate(model, loss_fn, val_loader, metric=accuracy)\n",
    "print(val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the training set and the validation set are very similar at this point - this is a good thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_fn, opt, train_dl, valid_dl, metric=None):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for xb, yb in train_dl:\n",
    "            loss, _, _ = loss_batch(model, loss_fn, xb, yb, opt)\n",
    "            \n",
    "        result = evaluate(model, loss_fn, valid_dl, metric)\n",
    "        val_loss, total, val_metric = result\n",
    "        \n",
    "        if metric is None:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, epoch, val_loss))\n",
    "    \n",
    "        else:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}, {}: {:.4f}'.format(epoch+1, epochs, val_loss, metric.__name__, val_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MnistModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.8776, accuracy: 0.6502\n",
      "Epoch [2/5], Loss: 1.5771, accuracy: 0.7494\n",
      "Epoch [3/5], Loss: 1.3654, accuracy: 0.7821\n",
      "Epoch [4/5], Loss: 1.2129, accuracy: 0.7993\n",
      "Epoch [5/5], Loss: 1.0997, accuracy: 0.8127\n"
     ]
    }
   ],
   "source": [
    "fit(5, model, F.cross_entropy, optimizer, train_loader, val_loader, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see how it grows logistically - not really grows after some time - 85% is the cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MNIST(root = 'data/', train = False, transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12b4df550>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset\n",
    "plt.imshow(img[0], cmap= 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.unsqueeze(0).shape\n",
    "\n",
    "#make it 4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img, model):\n",
    "    xb = img.unsqueeze(0)\n",
    "    yb = model(xb)\n",
    "    _, preds = torch.max(yb, dim = 1 )\n",
    "    return preds[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image(img, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1307cc470>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[1]\n",
    "plt.imshow(img[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 100\n",
    "\n",
    "def make_pred(var):\n",
    "    t, l = test_dataset[var]\n",
    "\n",
    "    plt.imshow(t[0], cmap='gray')\n",
    "    print(predict_image(t[0], model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "2\n",
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for i in range(3):\n",
    "    make_pred(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
