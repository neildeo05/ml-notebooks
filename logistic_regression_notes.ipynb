{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression is a supervised regression model, but logistic regression is a supervised classification model\n",
    "#predicting the value of a certain image - classifying it based on training data\n",
    "\n",
    "import torch\n",
    "#The torchvision package consists of popular datasets, model architectures, and common image transformations \n",
    "#for computer vision.\n",
    "import torchvision\n",
    "#import the MNIST dataset, which has all of the images - very popular, like the iris dataset\n",
    "from torchvision.datasets import MNIST\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actually download the dataset inside the data directory\n",
    "dataset = MNIST(root='data/', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look in your ml-notebooks directory - there is a directory called data. This is where we will store all of \n",
    "#the datasets that need separate files. The linear regression model was trained on very small amounts of data\n",
    "#and they were both tensors, so its easier to just store the data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "#The dataset has 60,000 images! We will have an ample amount of training data\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "#this dows not need to be download\n",
    "test = MNIST(root='data/', train=False)\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<PIL.Image.Image image mode=L size=28x28 at 0x11B8B6630>, 5)\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAOEUlEQVR4nO3dcYwV5bnH8d8jLUalENSIG9Ha22Bym0YXQUJiU6lNG4sm0JhWiHFp2mRJLAk1jam2q5DUGxujNGoicaukWLlCFS3Y1EsNS/TemDSuSBVLW6mhdMuGFTWyxEQqPPePHZoVd95Zzpk5c+D5fpLNOWeenTOPx/0xc847c15zdwE49Z1WdwMAWoOwA0EQdiAIwg4EQdiBID7Vyo2ZGR/9AxVzdxtreVN7djO7xsz+Yma7zey2Zp4LQLWs0XF2M5sg6a+SviZpQNLLkha7+58S67BnBypWxZ59jqTd7v6Wux+WtF7SgiaeD0CFmgn7BZL+MerxQLbsY8ys28z6zay/iW0BaFIzH9CNdajwicN0d++V1CtxGA/UqZk9+4CkC0c9ni5pX3PtAKhKM2F/WdIMM/ucmU2UtEjS5nLaAlC2hg/j3f0jM1smaYukCZLWuPsbpXUGoFQND701tDHeswOVq+SkGgAnD8IOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgmjplM049cyaNStZX7ZsWW6tq6srue5jjz2WrD/44IPJ+vbt25P1aNizA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQzOKKpM7OzmS9r68vWZ88eXKZ7XzM+++/n6yfc845lW27neXN4trUSTVmtkfSsKQjkj5y99nNPB+A6pRxBt1X3P1ACc8DoEK8ZweCaDbsLun3ZvaKmXWP9Qtm1m1m/WbW3+S2ADSh2cP4K919n5mdJ+l5M/uzu784+hfcvVdSr8QHdECdmtqzu/u+7HZI0jOS5pTRFIDyNRx2MzvLzD5z7L6kr0vaWVZjAMrVzGH8NEnPmNmx5/lvd/+fUrpCy8yZkz4Y27hxY7I+ZcqUZD11Hsfw8HBy3cOHDyfrRePoc+fOza0VXetetO2TUcNhd/e3JF1WYi8AKsTQGxAEYQeCIOxAEIQdCIKwA0Fwiesp4Mwzz8ytXX755cl1H3/88WR9+vTpyXo29Jor9fdVNPx1zz33JOvr169P1lO99fT0JNe9++67k/V2lneJK3t2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCKZtPAQ8//HBubfHixS3s5MQUnQMwadKkZP2FF15I1ufNm5dbu/TSS5PrnorYswNBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIyznwRmzZqVrF977bW5taLrzYsUjWU/++yzyfq9996bW9u3b19y3VdffTVZf++995L1q6++OrfW7OtyMmLPDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANB8L3xbaCzszNZ7+vrS9YnT57c8Lafe+65ZL3oevirrroqWU9dN/7II48k13377beT9SJHjhzJrX3wwQfJdYv+u4q+875ODX9vvJmtMbMhM9s5atnZZva8mb2Z3U4ts1kA5RvPYfwvJV1z3LLbJG119xmStmaPAbSxwrC7+4uS3j1u8QJJa7P7ayUtLLkvACVr9Nz4ae4+KEnuPmhm5+X9opl1S+pucDsASlL5hTDu3iupV+IDOqBOjQ697TezDknKbofKawlAFRoN+2ZJS7L7SyRtKqcdAFUpHGc3syckzZN0rqT9klZI+o2kX0u6SNJeSd9y9+M/xBvruUIexl9yySXJ+ooVK5L1RYsWJesHDhzIrQ0ODibXveuuu5L1p556KllvZ6lx9qK/+w0bNiTrN954Y0M9tULeOHvhe3Z3zzur4qtNdQSgpThdFgiCsANBEHYgCMIOBEHYgSD4KukSnH766cl66uuUJWn+/PnJ+vDwcLLe1dWVW+vv70+ue8YZZyTrUV100UV1t1A69uxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EATj7CWYOXNmsl40jl5kwYIFyXrRtMqAxJ4dCIOwA0EQdiAIwg4EQdiBIAg7EARhB4JgnL0Eq1atStbNxvxm338rGidnHL0xp52Wvy87evRoCztpD+zZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIxtnH6brrrsutdXZ2Jtctmh548+bNDfWEtNRYetH/kx07dpTdTu0K9+xmtsbMhsxs56hlK83sn2a2I/tp7tsZAFRuPIfxv5R0zRjLf+7undnP78ptC0DZCsPu7i9KercFvQCoUDMf0C0zs9eyw/ypeb9kZt1m1m9m6UnHAFSq0bCvlvR5SZ2SBiXdl/eL7t7r7rPdfXaD2wJQgobC7u773f2Iux+V9AtJc8ptC0DZGgq7mXWMevhNSTvzfhdAeygcZzezJyTNk3SumQ1IWiFpnpl1SnJJeyQtrbDHtpCax3zixInJdYeGhpL1DRs2NNTTqa5o3vuVK1c2/Nx9fX3J+u23397wc7erwrC7++IxFj9aQS8AKsTpskAQhB0IgrADQRB2IAjCDgTBJa4t8OGHHybrg4ODLeqkvRQNrfX09CTrt956a7I+MDCQW7vvvtyTPiVJhw4dStZPRuzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIxtlbIPJXRae+ZrtonPyGG25I1jdt2pSsX3/99cl6NOzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIxtnHycwaqknSwoULk/Xly5c31FM7uOWWW5L1O+64I7c2ZcqU5Lrr1q1L1ru6upJ1fBx7diAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgnH2cXL3hmqSdP755yfrDzzwQLK+Zs2aZP2dd97Jrc2dOze57k033ZSsX3bZZcn69OnTk/W9e/fm1rZs2ZJc96GHHkrWcWIK9+xmdqGZbTOzXWb2hpktz5afbWbPm9mb2e3U6tsF0KjxHMZ/JOmH7v6fkuZK+r6ZfUHSbZK2uvsMSVuzxwDaVGHY3X3Q3bdn94cl7ZJ0gaQFktZmv7ZWUvqcUAC1OqH37GZ2saSZkv4gaZq7D0oj/yCY2Xk563RL6m6uTQDNGnfYzWySpI2SfuDuB4su/jjG3Xsl9WbPkf4kC0BlxjX0Zmaf1kjQ17n709ni/WbWkdU7JA1V0yKAMhTu2W1kF/6opF3uvmpUabOkJZJ+lt2mv9c3sAkTJiTrN998c7Je9JXIBw8ezK3NmDEjuW6zXnrppWR927ZtubU777yz7HaQMJ7D+Csl3STpdTPbkS37sUZC/msz+56kvZK+VU2LAMpQGHZ3/z9JeW/Qv1puOwCqwumyQBCEHQiCsANBEHYgCMIOBGFFl2eWurGT+Ay61KWcTz75ZHLdK664oqltF52t2Mz/w9TlsZK0fv36ZP1k/hrsU5W7j/kHw54dCIKwA0EQdiAIwg4EQdiBIAg7EARhB4JgnL0EHR0dyfrSpUuT9Z6enmS9mXH2+++/P7nu6tWrk/Xdu3cn62g/jLMDwRF2IAjCDgRB2IEgCDsQBGEHgiDsQBCMswOnGMbZgeAIOxAEYQeCIOxAEIQdCIKwA0EQdiCIwrCb2YVmts3MdpnZG2a2PFu+0sz+aWY7sp/51bcLoFGFJ9WYWYekDnffbmafkfSKpIWSvi3pkLvfO+6NcVINULm8k2rGMz/7oKTB7P6wme2SdEG57QGo2gm9ZzeziyXNlPSHbNEyM3vNzNaY2dScdbrNrN/M+pvqFEBTxn1uvJlNkvSCpP9y96fNbJqkA5Jc0k81cqj/3YLn4DAeqFjeYfy4wm5mn5b0W0lb3H3VGPWLJf3W3b9Y8DyEHahYwxfC2MhXmz4qadfooGcf3B3zTUk7m20SQHXG82n8lyT9r6TXJR3NFv9Y0mJJnRo5jN8jaWn2YV7qudizAxVr6jC+LIQdqB7XswPBEXYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Io/MLJkh2Q9PdRj8/NlrWjdu2tXfuS6K1RZfb22bxCS69n/8TGzfrdfXZtDSS0a2/t2pdEb41qVW8cxgNBEHYgiLrD3lvz9lPatbd27Uuit0a1pLda37MDaJ269+wAWoSwA0HUEnYzu8bM/mJmu83stjp6yGNme8zs9Wwa6lrnp8vm0Bsys52jlp1tZs+b2ZvZ7Zhz7NXUW1tM452YZrzW167u6c9b/p7dzCZI+qukr0kakPSypMXu/qeWNpLDzPZImu3utZ+AYWZflnRI0mPHptYys3skvevuP8v+oZzq7j9qk95W6gSn8a6ot7xpxr+jGl+7Mqc/b0Qde/Y5kna7+1vufljSekkLauij7bn7i5LePW7xAklrs/trNfLH0nI5vbUFdx909+3Z/WFJx6YZr/W1S/TVEnWE/QJJ/xj1eEDtNd+7S/q9mb1iZt11NzOGacem2cpuz6u5n+MVTuPdSsdNM942r10j0583q46wjzU1TTuN/13p7pdL+oak72eHqxif1ZI+r5E5AAcl3VdnM9k04xsl/cDdD9bZy2hj9NWS162OsA9IunDU4+mS9tXQx5jcfV92OyTpGY287Wgn+4/NoJvdDtXcz7+5+353P+LuRyX9QjW+dtk04xslrXP3p7PFtb92Y/XVqtetjrC/LGmGmX3OzCZKWiRpcw19fIKZnZV9cCIzO0vS19V+U1FvlrQku79E0qYae/mYdpnGO2+acdX82tU+/bm7t/xH0nyNfCL/N0k/qaOHnL7+Q9Ifs5836u5N0hMaOaz7l0aOiL4n6RxJWyW9md2e3Ua9/UojU3u/ppFgddTU25c08tbwNUk7sp/5db92ib5a8rpxuiwQBGfQAUEQdiAIwg4EQdiBIAg7EARhB4Ig7EAQ/w8+sGPVrnT8WgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "image, label = dataset[1]\n",
    "\n",
    "plt.imshow(image, cmap='gray')\n",
    "print(\"Label:\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "dataset = MNIST(root=\"data/\", train=True, transform=transforms.ToTensor())\n",
    "ten, lab = dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0039, 0.6039, 0.9922, 0.3529, 0.0000],\n",
      "         [0.0000, 0.5451, 0.9922, 0.7451, 0.0078],\n",
      "         [0.0000, 0.0431, 0.7451, 0.9922, 0.2745],\n",
      "         [0.0000, 0.0000, 0.1373, 0.9451, 0.8824],\n",
      "         [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]]])\n"
     ]
    }
   ],
   "source": [
    "#Print the pixels at the 10th to 15th y axis \n",
    "print(ten[:,10:15, 10:15])\n",
    "#1 represents white, and 0 represents black - the some of these values are just in between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12d530160>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(ten[0,10:15,10:15], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split set into 3 models - training set, validation set, test set\n",
    "#training set - used to train the model , compute the loss, adjust the weights \n",
    "#validation set - evaluate the model while training, adjust parameters like the learning rate, and pick the best model\n",
    "#test set - used to compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_indices(n, val_pct):\n",
    "    #split the val into a training dataset and a validation dataset\n",
    "    n_val = int(val_pct*n)\n",
    "    \n",
    "    #random images for the validation set and for the training dataset \n",
    "    idxs = np.random.permutation(n)\n",
    "    \n",
    "    #returns the first n_vals in a tuple, so basically a fraction of the entire dataset, and splits the rest for\n",
    "    #the training set\n",
    "    return idxs[n_val:], idxs[:n_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, val_indices = split_indices(len(dataset), 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomize the data in order to remove bias\n",
    "\n",
    "#sample randomly from indices\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = 100\n",
    "\n",
    "#randomize the data samples\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "train_loader= DataLoader(dataset, batches, sampler=train_sampler)\n",
    "\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "val_loader = DataLoader(dataset, batches, sampler=val_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model\n",
    "\n",
    "#The logistic regression model is very similar to the linear regression model\n",
    "\n",
    "#there are weights and there are biases - output is obtained by using matrix operations\n",
    "\n",
    "#we can use nn.Linear to crreate the model - and flattened to a vector of size 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "#the output of each image is a vector of size 10, with each element of the vector signifying the probability of \n",
    "#a particular target label - 0 to 9\n",
    "\n",
    "input_size = 28*28 #flatten the image\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "# model = nn.Linear(input_size, num_classes)\n",
    "\n",
    "#super similar to the linear regression model - but this time the inputs and the outputs are way more\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for imgs, labels in train_loader:\n",
    "#     print(labels)\n",
    "#     print(imgs.shape)\n",
    "#     outputs = model(imgs)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 10]) tensor([[-0.1289, -0.1340, -0.2707,  0.2450, -0.1168, -0.1602, -0.2772,  0.2774,\n",
      "          0.1608, -0.0946]])\n"
     ]
    }
   ],
   "source": [
    "#Since the linear method is expecting a vector, we can't just pass in a tensor that has 3 dimensions, \n",
    "#we need to flatten it out - we can do this by creating a model class that inherists from the nn.module class\n",
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        #putting -1 allows us to use any batch size\n",
    "        xb=xb.reshape(-1,784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "model = MnistModel()\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    outputs = model(images)\n",
    "    break\n",
    "    \n",
    "print(outputs.shape, outputs[:1].data) \n",
    "\n",
    "\n",
    "#the outputs are the random result of passing in the images into the model. However, we need them to represent\n",
    "#a probability, which is between 1 and 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax function\n",
    "#probability is boosted up, tries to push it towards one output\n",
    "#divide by sum to end up with probabilities\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "#these sum up to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 3, 3, 7, 7, 5, 6, 7, 7, 3, 3, 7, 7, 3, 3, 5, 3, 3, 3, 7, 3, 6, 3, 0,\n",
      "        7, 7, 3, 7, 6, 3, 6, 3, 0, 7, 3, 6, 7, 7, 7, 5, 7, 3, 3, 3, 0, 3, 1, 5,\n",
      "        3, 3, 7, 7, 3, 3, 7, 7, 3, 7, 3, 3, 3, 3, 3, 1, 7, 7, 3, 7, 3, 7, 7, 1,\n",
      "        6, 6, 7, 0, 3, 6, 7, 3, 3, 7, 7, 3, 3, 3, 3, 3, 7, 3, 0, 3, 3, 1, 7, 7,\n",
      "        7, 3, 1, 7])\n"
     ]
    }
   ],
   "source": [
    "max_probs, preds = torch.max(probs, dim=1)\n",
    "#index of the max probability\n",
    "print(preds)\n",
    "# print(max_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 0, 1, 5, 6, 9, 4, 0, 1, 7, 3, 1, 7, 2, 6, 5, 8, 4, 3, 0, 2, 3, 6,\n",
       "        7, 6, 8, 5, 4, 0, 6, 1, 5, 6, 7, 6, 2, 3, 3, 6, 8, 3, 7, 7, 3, 7, 6, 6,\n",
       "        8, 0, 8, 3, 8, 9, 0, 8, 1, 1, 3, 8, 5, 0, 5, 5, 4, 2, 1, 4, 7, 2, 3, 9,\n",
       "        5, 6, 6, 9, 3, 0, 8, 1, 8, 2, 3, 4, 7, 2, 8, 2, 6, 3, 4, 3, 1, 4, 2, 0,\n",
       "        8, 1, 7, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what the number is in the index\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(l1, l2):\n",
    "    #l1 is the predictions, and l2 is the labels\n",
    "    \n",
    "    #the == operator checks which if the numbers line up\n",
    "    \n",
    "    #in total, 8 of the 100 numbers were matching. This is pretty bad - especially when you calculate the ratio\n",
    "    return torch.sum(l1 == l2).item()/len(l1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the model has 8% accuracy\n",
    "accuracy(preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cannot be used for gradient descent - mse is differentiable but torch.max() is not differentiable\n",
    "#the accuracy function is not looking at the actual probabilty \n",
    "\n",
    "\n",
    "#a good metric for the loss function is corss entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3194, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross Entropy\n",
    "\n",
    "#find where there is a correct value and then take the negative natural log of that value, \n",
    "\n",
    "loss_fn = F.cross_entropy\n",
    "#instead of passing in preds, which is softmaxed, we pass in the bare value, and it performs softmax on it\n",
    "loss = loss_fn(outputs,labels)\n",
    "\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def calc_accuracy(val):\n",
    "    return math.exp(-val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09833089582707522"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#holy shit thats doo doo\n",
    "calc_accuracy(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.0257,  0.0337,  0.0204,  ...,  0.0341,  0.0189,  0.0088],\n",
      "        [ 0.0176,  0.0305,  0.0038,  ..., -0.0268, -0.0351,  0.0264],\n",
      "        [-0.0315, -0.0079,  0.0033,  ...,  0.0224,  0.0103, -0.0285],\n",
      "        ...,\n",
      "        [-0.0329, -0.0192,  0.0301,  ..., -0.0300,  0.0070,  0.0107],\n",
      "        [ 0.0302,  0.0211,  0.0099,  ...,  0.0109,  0.0038, -0.0194],\n",
      "        [ 0.0292, -0.0270,  0.0259,  ...,  0.0208,  0.0077, -0.0153]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-2.9360e-02, -1.2545e-02,  5.3037e-05, -1.3715e-04, -2.9421e-02,\n",
      "        -2.4006e-02,  3.5588e-02,  1.3543e-02, -2.7553e-02,  1.4015e-02],\n",
      "       requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "#weights and biases\n",
    "print(list(model.parameters()))\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.001\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the loss on a batch\n",
    "def loss_batch(model, loss_fn, xb, yb, opt=None, metric=None):\n",
    "    #define the intial predictions\n",
    "    preds = model(xb)\n",
    "    #calculate the loss\n",
    "    loss = loss_fn(preds, yb)\n",
    "#     print(loss)\n",
    "    \n",
    "    if opt is not None:\n",
    "        #evaluate the gradients\n",
    "        loss.backward()\n",
    "        #re-pass the parameters in the optimizer\n",
    "        opt.step()\n",
    "        #set the gradients back to zero\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    \n",
    "    metric_result = None\n",
    "    #a metric is either the accuracy function or the cross entropy function\n",
    "    if metric is not None:\n",
    "        #calculate the accuracy of the predictions and yb, which is the y value of the batch\n",
    "        \n",
    "        \n",
    "        #this is where we are calling the metric\n",
    "        metric_result = metric(preds, yb)\n",
    "        \n",
    "    return loss.item(), len(xb), metric_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, valid_dl, metric=None):\n",
    "    #no computation of gradients - because this is used for the validation dataset\n",
    "    with torch.no_grad():\n",
    "        results = [loss_batch(model, loss_fn, xb, yb, metric = metric) for xb,yb in valid_dl]\n",
    "        #calculate the loss on the batch, which returns the cross entropy loss, and the accuracy prediction\n",
    "#         print(results)\n",
    "        \n",
    "        losses, nums, metrics = zip(*results)\n",
    "        total = np.sum(nums)\n",
    "#         avg_loss = np.sum(np.multiply(losses, nums)) / total\n",
    "#         avg_metric = None\n",
    "#         if metric is not None:\n",
    "#             avg_metric = np.sum(np.multiply(metrics, nums)) / total\n",
    "            \n",
    "#     return avg_loss, total, avg_metric\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.sum(preds == labels).item()/len(labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-4c8dcb93a3d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "val_loss, total, val_acc = evaluate(model, loss_fn, val_loader, metric=accuracy)\n",
    "print(val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the training set and the validation set are very similar at this point - this is a good thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_fn, opt, train_dl, valid_dl, metric=None):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for xb, yb in train_dl:\n",
    "            loss, _, _ = loss_batch(model, loss_fn, xb, yb, opt)\n",
    "            \n",
    "        evaluate(model, loss_fn, valid_dl, metric)\n",
    "#         val_loss, total, val_metric = result\n",
    "        \n",
    "#         if metric is None:\n",
    "#             print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, epoch, val_loss))\n",
    "    \n",
    "#         else:\n",
    "#             print('Epoch [{}/{}], Loss: {:.4f}, {}: {:.4f}'.format(epoch+1, epochs, val_loss, metric.__name__, val_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MnistModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(5, model, F.cross_entropy, optimizer, train_loader, val_loader, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see how it grows logarithmically - not really grows after some time - 85% is kinda the cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MNIST(root = 'data/', train = False, transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "img = test_dataset\n",
    "print(img[20][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img[0][0].unsqueeze(0).shape\n",
    "\n",
    "#make it 4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img, model, label):\n",
    "    xb = img.unsqueeze(0)\n",
    "    yb = model(xb)\n",
    "    print(yb)\n",
    "    print(label)\n",
    "    prob, preds = torch.max(yb, dim = 1 )\n",
    "    return preds[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0205, -0.9821, -0.4083, -0.0817,  0.0451, -0.2904, -0.9847,  2.5055,\n",
      "         -0.0439,  0.8031]], grad_fn=<AddmmBackward>)\n",
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image(img[0][0], model, img[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 7, 6, 8, 2, 4, 3, 5, 0, 2, 9, 0, 5, 8, 3, 6, 9, 2, 7, 5, 0, 6, 0, 4,\n",
      "        4, 1, 8, 3, 9, 8, 7, 1, 8, 2, 7, 3, 2, 4, 5, 2, 0, 4, 0, 9, 2, 8, 9, 7,\n",
      "        2, 3, 3, 3, 2, 6, 3, 2, 4, 5, 8, 2, 3, 5, 1, 9, 8, 7, 0, 1, 3, 0, 8, 4,\n",
      "        9, 2, 0, 1, 6, 2, 1, 4, 4, 7, 8, 2, 4, 6, 8, 4, 6, 9, 4, 9, 7, 3, 5, 0,\n",
      "        6, 4, 1, 6])\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "for xb,yb in train_loader:\n",
    "    print(yb)\n",
    "    break\n",
    "    \n",
    "print(img[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizes the loss on a batch\n",
    "def ez_loss_batch(xb, yb, loss_fn, optimizer, model):\n",
    "    #calculates the loss on a batch\n",
    "    preds = model(xb)\n",
    "    loss = loss_fn(preds, yb)\n",
    "    acc = accuracy(preds, yb)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1076598167419434 0.81\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in train_loader:\n",
    "    ez_loss_batch(xb,yb,loss_fn, optimizer, model)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8320e-01,  9.0924e-02,  8.9250e-03, -7.5423e-02, -3.2255e-01,\n",
       "         -5.4321e-01, -3.5003e-01, -1.2318e-01, -3.1722e-01, -1.8133e-01],\n",
       "        [-1.9981e-01,  1.4465e-01,  1.9669e-01, -2.6663e-02, -1.2637e-01,\n",
       "         -3.6504e-01, -1.5448e-01, -1.0127e-01, -2.9043e-01, -5.9738e-01],\n",
       "        [-2.8083e-01, -3.4996e-02, -8.9449e-02, -2.7711e-01, -3.7610e-01,\n",
       "         -2.9706e-01, -6.1263e-02, -2.6383e-01,  1.0358e-01, -3.8431e-02],\n",
       "        [-2.3138e-01, -5.1143e-02,  2.0211e-01, -2.2754e-01, -4.6536e-02,\n",
       "         -4.3747e-01, -1.0899e-01,  9.3485e-02, -5.3433e-02, -6.6090e-02],\n",
       "        [-1.7884e-01,  3.0499e-02,  1.1364e-01, -1.9247e-01, -6.8289e-02,\n",
       "         -7.3506e-01, -3.8459e-01, -8.7006e-02, -2.2231e-01, -1.9793e-01],\n",
       "        [-1.3670e-01,  8.9750e-02,  2.0861e-01, -4.1324e-01, -2.5401e-01,\n",
       "         -4.1240e-01, -2.3650e-01, -1.8981e-01,  5.8638e-03,  6.0321e-02],\n",
       "        [ 1.5669e-01,  1.6643e-01,  1.4494e-01,  9.2932e-02, -2.8723e-01,\n",
       "         -3.5897e-01, -2.4283e-01,  6.0005e-02, -2.9059e-01, -1.8721e-01],\n",
       "        [ 7.3850e-02,  4.2311e-02,  3.3149e-01, -8.0158e-02,  3.8295e-03,\n",
       "         -5.4528e-01, -1.0781e-01, -5.6720e-01, -3.4663e-01, -3.0618e-01],\n",
       "        [-2.8478e-01, -3.0154e-01,  2.7986e-02, -1.7882e-02, -6.8246e-02,\n",
       "         -1.9153e-01, -1.7845e-01, -1.5929e-01,  8.1571e-02,  1.4894e-01],\n",
       "        [-1.3681e-01, -1.4981e-01,  1.7829e-01, -3.5085e-02, -8.6211e-02,\n",
       "         -8.7853e-02, -1.0303e-01, -1.1889e-01, -1.7414e-02,  9.3138e-02],\n",
       "        [-3.0804e-01, -2.2917e-01, -1.7180e-01, -8.3135e-02,  9.7786e-02,\n",
       "         -2.1413e-01,  5.5833e-02,  1.3222e-01, -1.8647e-01,  1.9016e-01],\n",
       "        [ 1.7813e-02,  3.6705e-02,  5.7423e-02, -1.7127e-01,  3.5205e-02,\n",
       "         -1.8361e-01,  4.3071e-02, -1.2135e-01, -2.1822e-01,  7.4496e-02],\n",
       "        [-2.4779e-01, -8.9231e-02, -1.4317e-01, -2.0312e-01, -7.4835e-02,\n",
       "         -4.6007e-01, -4.5673e-03, -2.8450e-01, -2.6192e-01, -2.4203e-01],\n",
       "        [ 2.9336e-02, -1.2347e-02,  1.1849e-01, -2.7211e-02, -9.9281e-03,\n",
       "         -1.5916e-02, -3.2778e-02, -8.1261e-02, -1.4611e-01,  9.2723e-02],\n",
       "        [-1.9950e-01,  1.1217e-01,  7.4293e-03, -3.5006e-01, -3.8287e-02,\n",
       "         -8.1984e-01, -2.0256e-01, -1.3316e-02, -3.0839e-01, -1.6359e-01],\n",
       "        [ 5.7366e-02, -1.0452e-01,  1.3621e-01, -5.0564e-02,  4.4754e-02,\n",
       "         -1.1145e-01, -3.4388e-02, -3.3865e-02, -1.4081e-01, -3.1606e-02],\n",
       "        [-2.8424e-01,  6.5487e-02,  1.1108e-01, -3.3498e-01, -4.1316e-01,\n",
       "         -2.5734e-01, -1.1747e-01, -3.4497e-01, -8.4675e-02,  6.4793e-02],\n",
       "        [-1.7055e-02,  3.0112e-02, -1.0178e-01, -1.7166e-01,  9.3955e-03,\n",
       "         -3.1826e-01, -3.1315e-01, -1.1234e-01, -3.6363e-02, -2.3394e-01],\n",
       "        [-3.6590e-01,  5.9687e-02,  3.2504e-02, -2.4013e-01,  8.9128e-02,\n",
       "         -3.9373e-01, -2.1221e-02, -2.4493e-01, -3.6380e-01, -2.0358e-01],\n",
       "        [-3.4732e-01,  2.8519e-02, -2.6597e-01,  2.9269e-03, -8.0705e-02,\n",
       "         -1.0581e-01, -1.9919e-01,  4.4419e-02, -1.2533e-01, -1.3622e-01],\n",
       "        [-3.8325e-01, -7.3066e-02, -4.0140e-02, -3.6031e-01, -2.5931e-01,\n",
       "         -3.6731e-01, -3.0550e-01, -3.0346e-01, -1.8269e-01, -7.6247e-02],\n",
       "        [-2.5123e-01, -6.6416e-04,  8.0136e-02,  2.4145e-01, -3.4728e-02,\n",
       "         -2.1993e-01,  1.7094e-02, -3.4438e-01, -1.4706e-01,  5.0109e-02],\n",
       "        [-2.1032e-01, -1.2481e-01,  2.4624e-01, -2.5029e-01, -2.2364e-02,\n",
       "         -4.2539e-01, -6.1577e-02, -4.3718e-03, -8.4287e-02, -1.1277e-01],\n",
       "        [-2.8553e-01, -3.1293e-01, -1.3328e-01,  4.9414e-02, -2.6444e-01,\n",
       "         -1.3338e-01, -1.3377e-01, -7.9898e-02, -2.8842e-01,  1.1409e-01],\n",
       "        [ 1.0795e-01,  1.8444e-01,  2.9100e-02, -1.4924e-01, -7.0876e-02,\n",
       "         -5.7985e-01,  1.7381e-02, -1.3889e-01, -4.0027e-01, -3.1837e-01],\n",
       "        [-2.6228e-01, -1.4408e-01,  3.5382e-01,  4.9069e-02, -7.1218e-02,\n",
       "         -5.7595e-01, -3.0237e-01, -1.9556e-01, -4.3892e-02, -7.0222e-02],\n",
       "        [-2.2956e-01, -9.5223e-02, -1.5491e-01, -2.5436e-01, -1.6254e-01,\n",
       "         -3.9967e-01, -1.4907e-02, -6.5537e-02, -3.1890e-01, -7.1832e-02],\n",
       "        [-3.1981e-01, -4.8851e-02,  1.0539e-01, -3.0571e-01, -5.0516e-02,\n",
       "         -5.0404e-01, -1.9383e-01,  4.1213e-02, -2.4204e-01, -1.6673e-01],\n",
       "        [-2.7686e-01,  1.0107e-01,  1.1271e-01, -2.3986e-01, -9.5894e-02,\n",
       "         -7.2808e-01, -3.1325e-01, -2.9866e-01, -2.0432e-01, -2.8752e-02],\n",
       "        [-1.7456e-01, -3.4553e-02, -1.9075e-01, -1.8900e-01, -1.2453e-02,\n",
       "         -3.6720e-01, -3.4857e-01,  2.5370e-02, -1.8878e-01, -8.8058e-02],\n",
       "        [-4.3307e-02,  8.1800e-02,  1.9501e-01, -7.0819e-02,  1.5765e-01,\n",
       "         -4.5935e-01, -4.9894e-02,  4.0973e-02, -2.9435e-01, -1.0813e-01],\n",
       "        [-2.9694e-02,  3.4586e-01,  1.0170e-01, -1.7516e-01,  2.5029e-01,\n",
       "         -3.7006e-01, -2.3087e-02, -7.1137e-02, -8.7218e-02, -1.2666e-01],\n",
       "        [ 4.0161e-02,  2.1444e-01,  2.4110e-02, -9.5150e-02, -2.9458e-01,\n",
       "         -3.4780e-01, -1.3251e-01, -3.6359e-01, -2.3730e-01, -1.4029e-01],\n",
       "        [-1.0528e-01, -1.9205e-04,  1.7062e-01,  2.0422e-02, -2.6539e-01,\n",
       "         -1.4628e-02, -4.7778e-01, -3.8562e-01, -2.8482e-01, -3.2383e-01],\n",
       "        [-1.4349e-01, -2.3711e-01,  2.6318e-01, -9.1509e-02, -2.4413e-01,\n",
       "         -3.2842e-01, -3.8051e-01, -2.0312e-01, -2.8941e-01,  2.5223e-01],\n",
       "        [-3.1795e-01,  4.6550e-02,  1.2160e-01, -7.7642e-03,  2.1650e-01,\n",
       "         -2.8437e-01,  7.3765e-03, -1.2102e-01, -3.7647e-01, -1.3074e-01],\n",
       "        [-1.1044e-01,  5.8172e-02,  5.9577e-01,  4.7267e-02,  2.5986e-01,\n",
       "         -6.4196e-01, -2.5595e-01,  2.5494e-02, -4.8106e-02, -1.4490e-01],\n",
       "        [ 2.8485e-02,  2.4154e-01,  7.6617e-02, -3.5132e-01,  9.8079e-02,\n",
       "         -7.0231e-01, -1.0521e-01, -1.1875e-01, -1.7999e-01, -3.0222e-01],\n",
       "        [-2.6196e-02,  1.9024e-01,  1.1918e-01, -6.9377e-02, -8.1021e-02,\n",
       "         -4.4071e-01, -1.4721e-01, -1.3761e-01, -6.5377e-02, -1.0457e-01],\n",
       "        [-1.7557e-01, -2.2872e-01,  1.4302e-01, -3.2601e-03, -7.0926e-02,\n",
       "         -1.7921e-01, -1.6743e-01, -8.6279e-02, -8.1684e-02,  7.0505e-02],\n",
       "        [ 5.8337e-02, -5.8538e-02, -5.6457e-02, -3.5496e-02, -4.1466e-01,\n",
       "         -1.2769e-01,  1.1493e-01, -2.4607e-01, -8.9948e-02, -2.9466e-02],\n",
       "        [-3.9282e-01, -9.7581e-02, -1.0011e-01,  1.1595e-01, -8.9851e-02,\n",
       "         -3.4823e-01, -2.3242e-01, -2.2167e-01, -1.3702e-02, -7.5010e-02],\n",
       "        [-2.8912e-01,  3.7696e-02, -1.7373e-02, -2.2563e-02, -1.7415e-01,\n",
       "         -5.6770e-01, -3.9619e-01, -9.1807e-02, -3.0148e-01, -4.6452e-02],\n",
       "        [-3.0215e-01, -1.0608e-01,  2.5323e-01, -1.7852e-01,  6.1502e-02,\n",
       "         -1.1233e-01, -2.9690e-01, -1.0947e-01, -6.8705e-02,  6.6853e-03],\n",
       "        [-2.0975e-01,  7.0005e-02,  2.5512e-02, -7.1313e-02, -4.5464e-02,\n",
       "         -5.1735e-01, -2.4308e-02, -1.9668e-01, -4.8146e-01,  7.7278e-02],\n",
       "        [-2.3381e-01,  1.1981e-01, -1.2968e-01, -1.5044e-01, -7.7298e-02,\n",
       "         -3.4265e-01,  8.6384e-02,  8.5465e-02, -1.8381e-01, -3.3058e-01],\n",
       "        [-2.0446e-01, -2.8653e-01,  1.2021e-01, -1.1724e-01, -8.4606e-02,\n",
       "         -3.5458e-01, -1.2774e-01, -5.5386e-02, -9.1570e-02,  2.1672e-02],\n",
       "        [-9.2871e-02,  1.1733e-01, -1.3813e-01, -1.0630e-01, -1.4899e-01,\n",
       "         -2.1147e-01, -4.2369e-02, -3.1010e-01, -2.5880e-01, -1.6813e-01],\n",
       "        [-2.3470e-01, -4.4324e-02,  1.3944e-01, -4.0459e-02, -4.5924e-01,\n",
       "         -4.0618e-01, -3.4521e-01, -5.3371e-01, -1.1462e-01, -2.8394e-01],\n",
       "        [-1.2515e-01, -1.3444e-01, -1.1249e-01, -1.5647e-02,  1.3654e-01,\n",
       "         -3.4821e-01, -2.2870e-01, -1.5453e-01, -5.3320e-02,  2.1341e-02],\n",
       "        [-1.4788e-01,  9.1003e-02,  7.9330e-02, -1.6442e-01, -8.3584e-02,\n",
       "         -2.5479e-01,  1.0745e-01, -2.2852e-01, -2.6021e-01, -1.3170e-02],\n",
       "        [ 1.0077e-03, -2.4460e-01, -1.1700e-02,  8.4005e-02, -4.5451e-02,\n",
       "         -3.4620e-01, -1.4692e-01, -3.0271e-01, -7.4186e-02, -1.0145e-02],\n",
       "        [-3.1948e-01,  5.7962e-02,  1.4242e-01, -3.5188e-03,  1.1861e-01,\n",
       "         -1.7503e-01,  2.7853e-02,  2.2523e-01, -2.0849e-01, -7.9394e-02],\n",
       "        [ 8.2338e-02,  1.5606e-01,  4.5992e-02,  8.9373e-03, -1.6928e-01,\n",
       "         -3.5544e-01, -7.5766e-02, -2.6017e-01, -3.0345e-01, -8.9229e-02],\n",
       "        [-4.8139e-02, -2.4972e-02,  3.1926e-02, -5.2427e-01, -3.5911e-01,\n",
       "         -1.6301e-01, -2.0577e-01,  2.9407e-02,  1.0240e-01,  1.4509e-01],\n",
       "        [-2.7612e-01,  2.9313e-01,  1.0286e-01, -2.0462e-01,  1.3982e-01,\n",
       "         -2.7677e-01,  4.6304e-02,  6.8385e-02, -2.2222e-01, -3.6340e-02],\n",
       "        [-4.6587e-01, -1.9392e-01,  1.9021e-01,  1.0136e-01, -2.7183e-01,\n",
       "         -7.0517e-02, -6.5697e-01, -4.1425e-01,  1.4383e-01, -1.4908e-01],\n",
       "        [-2.1620e-02,  2.6020e-01,  2.6072e-01, -2.5988e-01, -1.4809e-01,\n",
       "         -3.6538e-01, -5.8285e-02, -2.5541e-01, -1.2197e-01, -8.3389e-02],\n",
       "        [-4.1893e-02, -1.1879e-01,  1.6785e-01,  1.8765e-01, -1.0410e-01,\n",
       "         -8.0525e-02, -1.0621e-02, -1.3717e-01, -2.3771e-01,  1.4885e-01],\n",
       "        [-3.4809e-01,  6.0117e-02,  4.2992e-01, -1.8678e-01, -1.0285e-01,\n",
       "         -4.6242e-01, -4.8028e-01, -3.4884e-01, -2.2034e-01, -5.1793e-01],\n",
       "        [-1.9479e-01, -1.0014e-01,  2.7951e-01, -2.9766e-02, -3.0566e-01,\n",
       "         -1.0506e-01, -1.9382e-01, -9.8394e-02, -2.8448e-01, -1.9211e-01],\n",
       "        [-2.6174e-01, -1.8671e-01,  1.2735e-01, -3.5826e-02,  5.7399e-02,\n",
       "         -3.7917e-01, -1.6510e-01, -2.1088e-01, -5.1964e-01, -3.2895e-01],\n",
       "        [-1.5578e-01,  1.4998e-01,  1.4282e-01, -1.4499e-02,  3.8269e-02,\n",
       "         -4.1243e-01, -3.5671e-01, -8.4870e-02, -4.8385e-02, -1.6265e-01],\n",
       "        [-7.8938e-02, -1.0455e-01,  3.4640e-01, -8.5891e-02, -4.0579e-03,\n",
       "         -5.1673e-01, -3.6493e-01, -2.8876e-01, -3.8221e-01,  1.4278e-01],\n",
       "        [-1.1327e-01,  1.2705e-01,  3.1398e-01, -3.1357e-01,  2.5132e-02,\n",
       "         -4.9312e-01, -2.8224e-01, -1.9815e-01, -1.9236e-01,  1.7729e-01],\n",
       "        [ 8.1567e-02,  4.2178e-02,  5.2670e-03, -1.4557e-01, -6.4196e-02,\n",
       "         -2.1164e-01, -5.2262e-02, -8.7360e-02, -2.6503e-01, -5.4599e-03],\n",
       "        [-3.2582e-01, -4.0543e-02,  2.4409e-01, -7.0612e-02, -1.8166e-01,\n",
       "         -1.5915e-01, -4.4140e-01, -1.9330e-01,  6.5922e-02, -1.1103e-01],\n",
       "        [-1.9590e-01,  2.6475e-02, -3.1748e-02, -2.9134e-01, -4.6749e-02,\n",
       "         -2.0822e-01, -2.3127e-01, -2.0305e-02, -1.4864e-01, -1.6186e-01],\n",
       "        [-1.6493e-01,  6.3050e-02, -7.1525e-02, -1.6949e-01, -2.3539e-01,\n",
       "         -3.3110e-01,  3.9609e-02, -2.0492e-02, -2.7615e-01, -1.2210e-01],\n",
       "        [-2.5578e-01, -1.7809e-01,  7.9730e-02,  9.0764e-02, -9.2125e-02,\n",
       "         -2.4870e-01,  1.1785e-01, -2.6245e-03, -1.5188e-01,  1.3858e-01],\n",
       "        [-1.4297e-01,  8.4022e-02,  1.3894e-01, -1.5206e-01, -1.3134e-01,\n",
       "         -4.3469e-01,  3.5231e-02, -4.0718e-01, -8.8073e-02, -2.3729e-01],\n",
       "        [-2.6945e-01,  1.1983e-02, -3.8224e-02, -3.5275e-02,  6.4609e-02,\n",
       "         -2.3805e-01, -8.8117e-02, -5.2526e-03, -1.3897e-01, -2.0665e-02],\n",
       "        [-4.3998e-01, -9.8488e-02, -2.4456e-01,  1.0742e-01, -1.6264e-01,\n",
       "         -1.4022e-01, -2.1630e-01, -1.6859e-01, -3.3247e-02, -1.5970e-01],\n",
       "        [-1.6064e-01, -1.5063e-01,  2.0629e-01, -7.6524e-02,  2.0203e-01,\n",
       "         -6.0279e-01, -4.0238e-01, -5.5292e-02, -2.6471e-01, -1.0444e-01],\n",
       "        [-1.4251e-01, -7.2507e-02,  1.0109e-01,  2.0681e-01, -1.5598e-01,\n",
       "         -1.1499e-02, -2.2900e-01, -3.3482e-01, -2.0425e-01,  1.4064e-01],\n",
       "        [-2.3463e-01, -3.4249e-02,  1.7359e-01, -2.0091e-01, -2.3905e-02,\n",
       "         -3.9076e-01, -1.6380e-01,  1.5503e-02, -5.7698e-02, -6.4634e-02],\n",
       "        [-1.1390e-01, -1.1848e-01,  5.7224e-02, -2.2926e-01, -2.3355e-01,\n",
       "         -3.0803e-01, -1.0109e-01, -2.3324e-01, -5.7284e-02,  2.0429e-01],\n",
       "        [-2.1750e-01, -2.5894e-01, -2.5362e-03, -1.6465e-01, -9.4185e-02,\n",
       "         -1.7696e-01, -8.3300e-02, -2.5585e-01, -1.0441e-01, -3.4058e-02],\n",
       "        [-1.6119e-01,  2.4284e-01, -4.7551e-02,  1.0590e-01,  3.9415e-03,\n",
       "         -2.8395e-01, -3.3580e-01, -2.2867e-03, -3.9502e-01, -3.4789e-01],\n",
       "        [-1.8170e-01, -1.1332e-01, -1.7868e-01, -1.8992e-01, -1.6079e-01,\n",
       "         -3.2615e-01, -5.7663e-02,  1.0044e-01, -1.2084e-01, -1.0553e-01],\n",
       "        [-6.1294e-03,  5.7411e-02, -1.9824e-02, -2.3012e-01, -1.2700e-01,\n",
       "         -5.7672e-01, -6.4832e-02, -1.5796e-01, -1.8517e-01, -2.8306e-03],\n",
       "        [-1.2003e-01,  7.9046e-02, -3.2434e-01, -1.7982e-01, -2.1215e-02,\n",
       "         -3.6104e-01, -1.0353e-01, -8.7919e-02, -2.0672e-01, -8.7387e-02],\n",
       "        [-1.7656e-01, -1.9610e-01,  2.1665e-01, -1.7922e-01,  3.3367e-02,\n",
       "         -1.1898e-01, -1.6009e-01, -1.4303e-01, -8.5572e-02,  9.3089e-02],\n",
       "        [-3.3693e-01, -3.5193e-02, -3.2355e-01, -1.9767e-01, -3.1197e-01,\n",
       "         -3.1108e-01,  5.6341e-02, -5.8671e-02, -2.1606e-01, -5.5121e-02],\n",
       "        [-1.0956e-01,  3.8330e-02, -1.7138e-01, -2.3500e-02,  1.2969e-01,\n",
       "         -4.5011e-01, -1.1042e-01, -1.4682e-02, -9.4842e-02, -4.5129e-03],\n",
       "        [-3.2345e-01, -5.9615e-02,  4.5956e-01, -1.3219e-02, -7.5680e-02,\n",
       "         -2.4570e-01, -7.4768e-01, -5.1720e-01, -1.2597e-01, -4.1793e-01],\n",
       "        [-2.5143e-01, -1.1931e-01, -1.0967e-01,  2.5723e-01, -2.1371e-01,\n",
       "          2.2057e-01, -1.6992e-02, -1.8023e-01,  2.5185e-02, -1.1336e-01],\n",
       "        [-8.3218e-02,  1.0261e-01,  1.8039e-01,  6.1013e-02, -2.6488e-02,\n",
       "         -5.1385e-01, -3.2676e-01, -3.1920e-01, -4.4900e-01, -3.5868e-01],\n",
       "        [-6.7193e-02,  1.1789e-02, -7.2633e-02, -8.9358e-02,  4.2345e-02,\n",
       "         -5.0847e-01, -2.7524e-02,  2.0375e-01, -1.1518e-01,  4.3798e-02],\n",
       "        [-2.6312e-01,  1.3963e-01,  1.4982e-01, -9.0888e-02, -4.6257e-02,\n",
       "         -3.4269e-01, -2.3583e-01, -2.3796e-01, -1.0540e-01, -1.0743e-01],\n",
       "        [-7.3259e-02, -7.6672e-02, -4.8257e-04,  4.6243e-02, -7.8896e-02,\n",
       "         -1.8735e-01, -4.2209e-02,  1.4864e-01, -2.1073e-01, -1.3062e-01],\n",
       "        [-6.2237e-02,  7.7752e-02, -1.0748e-01, -3.2953e-01, -1.5362e-01,\n",
       "         -2.3928e-01,  1.3847e-01,  1.6366e-02, -2.1829e-01,  2.0338e-01],\n",
       "        [-4.9518e-02, -1.5018e-01,  1.2144e-01, -3.9448e-02,  1.8488e-01,\n",
       "         -4.0565e-01, -1.1884e-01,  1.2689e-01, -5.9139e-02,  1.9146e-01],\n",
       "        [-4.2046e-01,  3.6206e-02,  2.1083e-01, -1.8417e-01, -1.8208e-01,\n",
       "         -2.7436e-01, -2.8238e-02, -7.0191e-02, -1.1232e-01,  5.4715e-02],\n",
       "        [-3.3365e-01,  2.0570e-01,  9.5493e-02, -2.3897e-02, -2.6353e-01,\n",
       "         -2.6787e-01, -1.8521e-01, -3.3706e-01, -3.1620e-01, -4.8682e-01],\n",
       "        [-1.5543e-01, -1.4515e-01,  4.3195e-02, -3.4772e-02,  1.0337e-01,\n",
       "         -2.1786e-01,  1.9744e-01, -8.2292e-02, -1.0490e-01,  8.1148e-03],\n",
       "        [-1.8338e-01, -1.5360e-01, -2.7357e-01, -2.8491e-02, -4.5592e-01,\n",
       "          3.7648e-02, -4.0733e-02, -1.0595e-01, -5.5880e-02,  3.5445e-02],\n",
       "        [-1.3561e-01, -3.9090e-02,  2.1661e-01,  1.3696e-02, -1.2424e-01,\n",
       "         -4.6387e-01,  3.6578e-02, -5.3018e-02, -5.1918e-01, -4.6834e-01],\n",
       "        [-3.1851e-01,  2.4808e-01,  1.0087e-01, -2.1582e-01,  3.5727e-02,\n",
       "         -4.9723e-01,  1.1579e-01, -2.1587e-02, -3.9438e-04, -1.2145e-01],\n",
       "        [-3.7852e-01, -6.8059e-02,  1.0019e-01, -1.4373e-01, -5.6593e-02,\n",
       "         -1.5771e-01, -7.7975e-02, -7.7523e-02,  2.0615e-02,  1.0138e-01]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        #putting -1 allows us to use any batch size\n",
    "        xb=xb.reshape(-1,784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "mdl = MnistModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizes the loss on a batch\n",
    "\n",
    "#what is currently occuring is that we have 12 percent accuracy on our model\n",
    "def ez_loss_batch(xb, yb, loss_fn, optimizer, model):\n",
    "    #calculates the loss on a batch\n",
    "    preds = model(xb)\n",
    "    loss = loss_fn(preds, yb)\n",
    "    acc = accuracy(preds, yb)\n",
    "#     print(loss.item(), acc)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "\n",
    "for epoch in range (5):\n",
    "    for xb, yb in train_loader:\n",
    "        ez_loss_batch(xb,yb,loss_fn, optimizer, mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1870, -0.1386,  0.0695,  0.1307,  0.1137, -0.2552,  0.0220,  0.0187,\n",
      "         -0.0630,  0.0314]], grad_fn=<AddmmBackward>)\n",
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image(img[0][0],mdl, img[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = img.unsqueeze(0)\n",
    "yb = model(xb)\n",
    "print(yb)\n",
    "print(label)\n",
    "prob, preds = torch.max(yb, dim = 1 )\n",
    "return preds[0].item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
